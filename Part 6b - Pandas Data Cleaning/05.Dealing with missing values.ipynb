{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Missing Data\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lesson, we'll learn about various strategies for detecting and dealing with missing data using Pandas.  \n",
    "\n",
    "## Objectives\n",
    "You will be able to:\n",
    "* Detect missing data in Pandas using .describe(), .summary(), .isnull and .notnull\n",
    "* Replace/drop missing data in Pandas using .fillna and .dropna\n",
    "\n",
    "\n",
    "## Why is Missing Data a Problem?\n",
    "\n",
    "Missing data can be problematic during the data science process because null values in our dataset limit our ability to do important things like:\n",
    "\n",
    "* Convert data types\n",
    "* Calculate summary statistics\n",
    "* Visualize Data\n",
    "* Build models\n",
    "\n",
    "## Detecting Missing Data\n",
    "\n",
    "There are two main ways missing data is often represented in data sets.\n",
    "\n",
    "### NaNs\n",
    "\n",
    "By default, pandas represents null values with `NaN`, which is short for **_Not a Number_**.  Pandas provides many great ways for checking for null values, built right into DataFrames and Series objects.\n",
    "\n",
    "#### Example--Detecting Null Values\n",
    "\n",
    "```python\n",
    "df.isna()\n",
    "```\n",
    "\n",
    "Returns a matrix of boolean values, where all cells containing `NaN` are converted to `True`, and all cells containing valid data are converted to `False`\n",
    "\n",
    "```python\n",
    "df.isna().sum()\n",
    "```\n",
    "\n",
    "Since `True` is equivalent to `1` and `False` is equivalent to `0` in python, taking the `.sum()` of the DataFrame (or Series) will return the total number of `NaN` values in the dataset.  Pandas even breaks this down by column--see the example output below.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Null Values in Titanic Dataset by Column\n",
    "\n",
    "PassengerId      0\n",
    "Survived         0\n",
    "Pclass           0\n",
    "Name             0\n",
    "Sex              0\n",
    "Age            177\n",
    "SibSp            0\n",
    "Parch            0\n",
    "Ticket           0\n",
    "Fare             0\n",
    "Cabin          687\n",
    "Embarked         2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder Values\n",
    "\n",
    "Often, datasets will contain missing values that are denoted by a value that seems valid to pandas.  This is very common in real-world datasets--often, people entering the data are required to enter values that they don't actually have, so they enter an agreed-upon placeholder value.  \n",
    "\n",
    "The easiest way to deal with these is to familiarize yourself with the data dictionary that corresponds to your data set--any placeholder values meant to denote a null value will be specified here. \n",
    "\n",
    "However, you'll encounter plenty of data sets in the real world that don't come with a data dictionary, or that fail to mention placeholder values.  \n",
    "\n",
    "Follow these strategies for detecting placeholder values in your data. \n",
    "\n",
    "\n",
    "#### Numerical Data\n",
    "\n",
    "Numerical columns will often represent missing values with a value that is nonsensical to the column in question.  For instance, in healthcare data, missing values in a `Weight` column may be using impossible values such as `0` or `9999`.  These are valid to the computer, since they are real-numbered, but are obvious to anyone analyzing the data as placeholder values.  \n",
    "\n",
    "These are the most difficult to check for, since this requires getting decently familiar with the column in question enough to notice values that are technically valid but pragmatically impossible. \n",
    "\n",
    "To detect these sorts of placeholder values, start by checking for outliers--they are often encoded as very large numbers, or as 0 (when 0 isn't a realistic value for the column in question).  Any good placeholder value will be a value that couldn't show up in the real world.  \n",
    "\n",
    "Another way to confirm these values is to check the `value_counts`.  In a continuously-valued column, it is probably rare for one specific value to overwhelm all the others.  If, for instance, you see the same numerical value showing up a statistically improbable number of times, double-check that this value is real--placeholder values have the potential to show up many times, but it's much less likely for real-valued numbers.  \n",
    "\n",
    "\n",
    "#### Categorical Data\n",
    "\n",
    "To detect placeholder values in categorical data, get the unique values in the column and see if there are any values that don't match up with your expectations.  Pandas provides a built-in method for this.  For instance, in the titanic dataset, we can check the unique values of the `Embarked` column by typing:\n",
    "\n",
    "```python\n",
    "df['Embarked'].unique()\n",
    "```\n",
    "\n",
    "This will return an array containing all the unique values in the dataset.  \n",
    "\n",
    "Note that for categorical columns, it is much more likely to have a data dictionary to work with, since it is common to have categorical values that aren't readily understandable without a data dictionary to help us figure out what each potential category means.  \n",
    "\n",
    "## Strategies For Dealing with Missing Data\n",
    "\n",
    "Detecting missing values isn't enough--we need to deal with them in order to move forward! We have 3 options for dealing with null values--removing them from the data set, keeping them, or replacing them with another value. \n",
    "\n",
    "### Remove\n",
    "\n",
    "The easiest way to deal with null values is to drop the offending rows and/or columns.  The downside to this is that we lose data in the process.  This is a valid strategy on very large datasets--however, on smaller datasets, throwing away data may be unacceptable.  \n",
    "\n",
    "The two main strategies for dealing with null values are to drop columns or to drop rows.  For this strategy, it does not matter if we are dealing with continuous or categorical data.  \n",
    "\n",
    "#### Dropping Columns\n",
    "\n",
    "Consider the output from the titanic dataset shown previously.  The `Cabins` column contains 687 missing values. The entire dataset only contains around 900 rows of data.  In this case, it makes more sense to just remove the `Cabins`  column from the dataset entirely.  \n",
    "\n",
    "Note that while this makes sense for the `Cabins` column, this is not a good idea for dealing with the null values contained within the `Age` column. Although the `Age` column contains 75 missing values, the vast majority of the items in this dataset contain perfectly good information for the age column.  If we dropped this column, we would be throwing out all that information just to deal with a small subset of missing values in that column!\n",
    "\n",
    "#### Dropping Rows\n",
    "\n",
    "In the above example, dropping all rows that contain a null value would be a very bad idea, because we would 3/4 of our data! Dropping rows makes more sense when the proportion of rows with missing values is very small compared to the size of the overall data set--it's okay to just throw out the missing values as long as it's not too many observations. There's no hard rule for exactly how many missing values is the right amount to throw out, and will vary project by project.  Think critically, and use your best judgment!\n",
    "\n",
    "To drop all rows containing missing values in a DataFrame, use `dataframe.dropna()`.  Note that this returns a copy of the dataframe with the rows in question dropped--however, you can mutate the DataFrame in place by passing in `inplace=True` as a parameter to the method call. \n",
    "\n",
    "### Replace\n",
    "\n",
    "We can also deal with missing values by replacing them with a common value.  The downside of this method is that this can introduce noise into our dataset. \n",
    "\n",
    "#### Continuous Data\n",
    "\n",
    "For continuous data, the best solution is to replace the missing values with the median value for that column.  The median value is a good choice because it is least likely to influence the distribution of the dataset overall.  If the dataset is symmetric, then the mean and the median will be the same value.  If the dataset is not symmetric, then the mean is more likely to be skewed by outlier values, so the median is a better choice.  \n",
    "\n",
    "Pandas provides an easy way for us to replace null values.  For instance, if we wanted to replace all null values in the `Fare` column with the column median, we would type:\n",
    "\n",
    "```python\n",
    "df['Fare'].fillna(df['Fare'].median())\n",
    "```\n",
    "\n",
    "#### Categorical Data\n",
    "\n",
    "With categorical data, this is harder, since we don't have summary statistics to lean on such as the median or the mean. In this case, if one categorical value is much more common than others, it is a valid strategy to replace null values with this common value. However, make sure to examine your data first! If all the categorical values are equally common, picking one to replace all the null values may do more harm than good by skewing the distribution and introducing some false signal into your dataset.\n",
    "\n",
    "### Keep \n",
    "\n",
    "Sometimes, the knowledge that a value is missing can itself be informative for us.  If knowing that a value is missing tells you something, then it is often worth keeping the null values using the following strategies. \n",
    "\n",
    "#### Categorical Data\n",
    "\n",
    "This one is the easiest--just treat missing values as its own category! This may require replacing null values with a string to denote this, as your model will still likely throw errors if the actual `NaN` values are not replaced. In that case, just replace the `NaN` values with the string `'NaN'`, or another string that makes it obvious that this value is `'missing'`.\n",
    "\n",
    "\n",
    "#### Numerical Data\n",
    "\n",
    "Often, null values inside a continuously-valued column will cause all sorts of havoc in your models, so leaving the `NaN`s alone isn't usually an option here.  Instead, consider using **_Coarse Classification_**, also referred to as **_Binning_**.  This allows us to convert the entire column from a numerical column to a categorical column by binning our data into categories.  For instance, we could deal with the missing values in the `Age` column by creating a categorical column that separates each person into 10-year age ranges.  Anybody between the ages of 0 and 10 would be a `1`, 11 to 20 would be a `2`, and so on.  \n",
    "\n",
    "Once we have binned the data in a new column, we can throw out the numerical version of the column, and just leave the missing values as one more valid category inside our new categorical column!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Missing Data - Exercise\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, we'll work through strategies for data cleaning and dealing with null values (NaNs).\n",
    "\n",
    "## Objectives\n",
    "* Detect missing data in Pandas using .describe(), .info(), .isnull and .notnull\n",
    "* Replace/drop missing data in Pandas using .fillna and .dropna\n",
    "\n",
    "\n",
    "## Dataset\n",
    "\n",
    "In this lab, we'll continue working with the _Titanic Survivors_ dataset, which can be found in `titanic.csv`.\n",
    "\n",
    "Before we can get going, we'll need to import the usual libraries.  In the cell below, import:\n",
    "* `pandas` as `pd`\n",
    "* `numpy` as `np`\n",
    "* `matplotlib.pyplot` as `plt`\n",
    "* set `%matplotlib inline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries below\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  PassengerId  Survived Pclass  \\\n",
       "0           0            1         0      3   \n",
       "1           1            2         1      1   \n",
       "2           2            3         1      3   \n",
       "3           3            4         1      1   \n",
       "4           4            5         0      3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/titanic.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Null Values in a DataFrame\n",
    "\n",
    "Before we can deal with null values, we first need to find them. There are several easy ways to detect them.  We will start by answering very general questions, such as \"does this DataFrame contain any null values?\", and then narrowing our focus each time the answer to a question is \"yes\".\n",
    "\n",
    "We'll start by checking to see if the DataFrame contains **any** null values (NaNs) at all. \n",
    "\n",
    "**_Hint_**: If you do this correctly, it will require method chaining, and will return a boolean value for each column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0     False\n",
       "PassengerId    False\n",
       "Survived       False\n",
       "Pclass         False\n",
       "Name           False\n",
       "Sex            False\n",
       "Age             True\n",
       "SibSp          False\n",
       "Parch          False\n",
       "Ticket         False\n",
       "Fare           False\n",
       "Cabin           True\n",
       "Embarked        True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now we know which columns contain null values, but not how many. \n",
    "\n",
    "In the cell below, check chain a different method with `isna()` to check how many total null values are in each column.  \n",
    "\n",
    "Expected Output:\n",
    "\n",
    "```\n",
    "PassengerId      0\n",
    "Survived         0\n",
    "Pclass           0\n",
    "Name             0\n",
    "Sex              0\n",
    "Age            177\n",
    "SibSp            0\n",
    "Parch            0\n",
    "Ticket           0\n",
    "Fare             0\n",
    "Cabin          687\n",
    "Embarked         2\n",
    "dtype: int64```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0       0\n",
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how many null values exist in each column, we can make some decisions about how to deal with them.  \n",
    "\n",
    "We'll deal with each column individually, and employ a different strategy for each.  \n",
    "\n",
    "\n",
    "### Dropping the Column\n",
    "\n",
    "The first column we'll deal with is the `Cabin` column.  We'll begin by examining this column more closely. \n",
    "\n",
    "\n",
    "In the cell below:\n",
    "* Determine what percentage of rows in this column contain missing values\n",
    "* Print out the number of unique values in this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "687"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total number of nulls in Cabin \n",
    "len(df[df['Cabin'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7710437710437711"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding the % of null values in cabin\n",
    "len(df[df['Cabin'].isna()])/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Cabin.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note , we can also find missing values this way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 866 missing values in the dataframe\n"
     ]
    }
   ],
   "source": [
    "print(\"There is {} missing values in the dataframe\".format(df.isnull().sum().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the percentage missing data in the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Age</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Parch</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Name</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Survived</th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>687.000000</td>\n",
       "      <td>177.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Percent</th>\n",
       "      <td>77.104377</td>\n",
       "      <td>19.86532</td>\n",
       "      <td>0.224467</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Cabin        Age  Embarked  Fare  Ticket  Parch  SibSp  Sex  \\\n",
       "Total    687.000000  177.00000  2.000000   0.0     0.0    0.0    0.0  0.0   \n",
       "Percent   77.104377   19.86532  0.224467   0.0     0.0    0.0    0.0  0.0   \n",
       "\n",
       "         Name  Pclass  Survived  PassengerId  Unnamed: 0  \n",
       "Total     0.0     0.0       0.0          0.0         0.0  \n",
       "Percent   0.0     0.0       0.0          0.0         0.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for % of missing values\n",
    "total = df.isnull().sum().sort_values(ascending = False)\n",
    "percent = (df.isnull().sum()/df.isnull().count()*100).sort_values(ascending = False)\n",
    "pd.concat([total, percent], axis=1, keys=['Total', 'Percent']).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ This way we can have an overview of all missing columns at once instead of looking for each column one at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this many missing values, it's probably best for us to just drop this column completely.\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* drop the `Cabin` column in place from the `df` DataFrame\n",
    "* Then, check the remaining number of null values in the data set by using the code you wrote previously.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Cabin' ,axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0       0\n",
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check results\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Placeholder Values\n",
    "\n",
    "Recall that another common strategy for dealing with null values is to replace them with the mean or median for that column.  We'll begin by investigating the current version of the `'Age'` column.  \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Plot a histogram of values in the `'Age'` column with 80 bins (1 for each year).   \n",
    "* Print out the mean and median for the column.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.699"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Age'].mean().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Age'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot a histogram to show the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Value for Age column:29.69911764705882\n",
      "Median Value for Age column:28.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "age_mean = df['Age'].mean()\n",
    "age_median = df['Age'].median()\n",
    "df['Age'].plot(kind='hist', bins=80)\n",
    "\n",
    "print('Mean Value for Age column:{}'.format(age_mean))\n",
    "print('Median Value for Age column:{}'.format(age_median))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the visualization above, we can see the data has a slightly positive skew. \n",
    "\n",
    "In the cell below, replace all null values in the `'Age'` column with the median of the column.  **Do not hard code this value--use the methods from pandas or numpy to make this easier!**  Do this replacement in place on the DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we know the median age we can fill it up in the Dataframe\n",
    "df['Age'] = df.Age.fillna(value=df.Age.median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Now that we've replaced the values in the `'Age'` column, let's confirm that they've been replaced.  \n",
    "\n",
    "+ In the cell below, check how many null values remain in the dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0     0\n",
       "PassengerId    0\n",
       "Survived       0\n",
       "Pclass         0\n",
       "Name           0\n",
       "Sex            0\n",
       "Age            0\n",
       "SibSp          0\n",
       "Parch          0\n",
       "Ticket         0\n",
       "Fare           0\n",
       "Embarked       2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check results\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we need to deal with the two pesky null values in the `'Embarked'` column.  \n",
    "\n",
    "### Dropping Rows That Contain Null Values\n",
    "\n",
    "Perhaps the most common solution to dealing with null values is to simply drop any rows that contain them.  Of course, this is only a good idea if the number dropped does not constitute a significant portion of our dataset.  Often, you'll need to make the overall determination to see if dropping the values is an acceptable loss, or if it is a better idea to just drop an offending column (e.g. the `'Cabin'` column) or to impute placeholder values instead.\n",
    "\n",
    "In the cell below, use the appropriate built-in DataFrame method to drop the rows containing null values. Do this in place on the DataFrame.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0     0\n",
       "PassengerId    0\n",
       "Survived       0\n",
       "Pclass         0\n",
       "Name           0\n",
       "Sex            0\n",
       "Age            0\n",
       "SibSp          0\n",
       "Parch          0\n",
       "Ticket         0\n",
       "Fare           0\n",
       "Embarked       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace= True)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We've dealt with all the **_obvious_** null values, but we should also take some time to make sure that there aren't symbols or numbers included that are meant to denote a missing value. \n",
    "\n",
    "### Missing Values with Placeholders\n",
    "\n",
    "A common thing to see when working with datasets is missing values denoted with a preassigned code or symbol.  Let's check to ensure that each categorical column contains only what we expect.\n",
    "\n",
    "In the cell below, return the unique values in the `'Embarked'`, `'Sex'`, `'Pclass'`, and `'Survived'` columns to ensure that there are no values in there that we don't understand or can't account for.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['S', 'C', 'Q'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Embarked.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['male', 'female'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Sex.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['3', '1', '2', '?'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Pclass.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Survived.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imagine if we have a lot of columns, we cannot go through them all one by one. So let's use a for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value for Embarked:\n",
      "['S' 'C' 'Q']\n",
      "\n",
      "\n",
      "Value for Sex:\n",
      "['male' 'female']\n",
      "\n",
      "\n",
      "Value for Pclass:\n",
      "['3' '1' '2' '?']\n",
      "\n",
      "\n",
      "Value for Survived:\n",
      "[0 1]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in ['Embarked','Sex','Pclass','Survived']:\n",
    "    print('Value for {}:\\n{}\\n\\n'.format(col,df[col].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It Looks like the `'Pclass'` column contains some missing values denoted by a placeholder! \n",
    "\n",
    "In the cell below, investigate how many placeholder values this column contains.  Then, deal with these null values using whichever strategy you believe is most appropriate in this case.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    0.527559\n",
       "1    0.224972\n",
       "2    0.193476\n",
       "?    0.053993\n",
       "Name: Pclass, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note there are 5% of ?\n",
    "df.Pclass.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#observation : ? accounts for 5% of the data\n",
    "#method : randomly select a class according to current distribution\n",
    "rel_prob = [.53,.22,.19]\n",
    "prob = [i/sum(rel_prob) for i in rel_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_pclass(value):\n",
    "    if value == '?':\n",
    "        return np.random.choice(['3','1','2'], p=prob)\n",
    "    else:\n",
    "        return value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    0.56018\n",
       "1    0.23622\n",
       "2    0.20360\n",
       "Name: Pclass, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Pclass = df.Pclass.map(lambda x: impute_pclass(x))\n",
    "df.Pclass.value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['3', '1', '2'], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking to see result.\n",
    "df.Pclass.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Question:_** What is the benefit of treating missing values as a separate valid category?  What is the benefit of removing or replacing them? What are the drawbacks of each? Finally, which strategy did you choose? Explain your choice below. \n",
    "\n",
    "Write your answer below this line:\n",
    "____________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample response:\n",
    "\n",
    "# By treating missing values as a separate category, information is preserved. \n",
    "# Perhaps there is a reason that this information is missing. \n",
    "# By removing or replacing missing information, we can more easily conduct mathematical analyses which require values for computation. \n",
    "# I chose to randomly replace for now. I could have just as easily removed the data. \n",
    "# Concerns include that I imputed the wrong value (indeed it was a random guess). \n",
    "# The strategy for dealing with missing data will depend on our desired application, \n",
    "# but regardless of the approach taken, the ramifications of how missing data are handled must be considered. \n",
    "# For example, imputing the median of our age reduces variance \n",
    "# and assumes that a new value would be close to the center of the distribution \n",
    "# (albeit this assumption is statistically likely)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Now, let's do a final check to ensure that there are no more null values remaining in this dataset.  \n",
    "\n",
    "In the cell below, reuse the code you wrote at the beginning of the notebook to check how many null values our dataset now contains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0     0\n",
       "PassengerId    0\n",
       "Survived       0\n",
       "Pclass         0\n",
       "Name           0\n",
       "Sex            0\n",
       "Age            0\n",
       "SibSp          0\n",
       "Parch          0\n",
       "Ticket         0\n",
       "Fare           0\n",
       "Embarked       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Great! Those all seem in line with our expectations.  We can confidently say that this dataset contains no pesky null values that will mess up our analysis later on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "+ In this section, we learned:\n",
    "\n",
    "* Strategies for detecting `NaN` values in pandas\n",
    "* Strategies for detecting missing data denoted by place holder values\n",
    "* How to deal with missing values by Removing, Replacing, or Keeping them!\n",
    "\n",
    "* How to detect null values in our dataset\n",
    "* How to deal with null values by dropping rows\n",
    "* How to deal with null values by imputing mean/median values \n",
    "* Strategies for detecting null values encoded with a placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
